# Samenhang en lineaire regressie {#linregressie}

:::{.chapterintro}
Voor het nemen van beslissingen is het van belang om te weten of er samenhang tussen variabelen bestaat. Ook kan het zijn dat je de waarde van een variabele wilt schatten op basis van de waarde van andere variabelen. Onderzoek naar een mogelijk verband tussen variabelen is dan nodig. Is er een verband dan kan een model voor het verband worden opgesteld.
:::

De variabele waarvan je de waarde wilt schatten en die afhangt van de waarde van een andere variabele wordt heeft benamingen als *afhankelijke variabele*, *verklaarde variabele* of *respons variabele*. En de variabelen waarvan deze afhangt worden de *onafhankelijke variabelen*, of *verklarende variabelen* genoemd. De techniek naar mogelijke verbanden wordt ook wel *regressie-analyse* genoemd.

## Samenhang {#samenhang}

De waarden van een variabele kunnen samenhangen met de waarden van een andere variabele. Zo zal meestal de benzineprijs stijgen wanneer de prijs van een vat ruwe olie stijgt. De variabelen benzineprijs en olieprijs vertonen dan  een positieve samenhang. Het omgekeerde kan ook, wanneer bijvoorbeeld de werkloosheid stijgt dan dalen de consumentenuitgaven. De variabelen werkloosheidspercentage en consumentenuitgaven vertonen een negatieve samenhang. 

Of er een mogelijk verband tussen numerieke variabelen is kun je onderzoeken

+ door ze in een spreidingsdiagram tegen elkaar uit te zetten en dan te kijken of er een patroon zichtbaar is.
+ door de covariatie of correlatiecoëfficiënt te berekenen.

wanneer er een verband tussen twee variabelen is, dan zal ook de variantie van de variabelen samenhang vertonen. Er is dan sprake van *covariantie* (gedeelde variantie). vandaar dat variabelen die aan elkaar gerelateerd zijn en dus een deel van hun variantie delen, ook wel *covariaten* genoemd worden. Een maatstaf om de samenhang tussen twee variabelen in uit te drukken is de *covariantie*, welke aangeeft of en hoeveel de waarden van de ene variabele toe- dan wel afnemen bij toenemende waarden van de andere variabele.

<!-- De covariatie is het gemiddelde van de produkten van de deviaties (afwijkingen van het gemiddelde) voor ieder gegevenspaar. -->

Het lastige is echter dat variantie afhangt van de meetschaal. Stap je bijvoorbeeld bij het meten van de lengte over van meters naar centimeters, dan worden de meetwaarden zelf 100 keer zo groot, maar de variantie wordt 10.000 keer zo groot. De variantie is dus gevoelig voor veranderingen in de schaal. Daarom kun je door simpelweg naar de covariantie te kijken niet zeggen of de variabelen veel of weinig van hun variantie delen. De covariantie is hierdoor een minder bruikbaar getal en zal dan ook verder bij de analyses niet bekeken worden.

Een betere maat voor de samenhang is de *Pearson correlatiecoëfficiënt* welke berekend wordt door de covariantie te delen door het produkt van de standaarddeviaties van beide variabelen. Deze is onafhankelijk van de meetschaal. Deze correlatiecoëfficiënt wordt meestal weergegeven met $r$ en de waarde loopt van -1 tot +1:

+ $-1$ bij een volledig negatieve samenhang.
+ $0$ bij geen samenhang
+ $+1$ bij een volledig positieve samenhang

Met de correlatiecoëfficiënt kun je dus zowel de sterkte van de correlatie aangeven als de richting (positief of negatief).

Voor alleen de sterkte van de correlatie wordt meestal gewerkt met het kwadraat van de correlatiecoëfficiënt, aangeduid met $R^2$. De benaming hiervoor is *R-kwadraat* of ook wel *determinatiecoefficient*. De waarde ervan loopt van 0 tot 1. Hoe dichter bij 1, hoe sterker het verband.

Deze R-kwadraat geeft tevens aan welk deel (of percentage bij vermenigvuldiging met 100) van de variantie de variabelen met elkaar delen. Of met andere woorden, welke deel van de variantie in de ene variabele verklaard wordt door de variantie in de andere variabele. Dit wordt de *verklaarde variantie* genoemd.

**Excel formules**

+ Covariantie populatie: `COVARIANTIE.P()`
+ Covariantie steekproef: `COVARIANTIE.S()`
+ Pearson correlatiecoëfficiënt: `PEARSON()`
+ R-kwadraat: `R-KWADRAAT()`

## Lineair verband

Wiskundig gezien is een verband tussen twee numerieke variabelen lineair wanneer je dat verband grafisch weer kunt geven met een rechte lijn. Bij een niet-lineair verband heb je een gebogen lijn.

Heb je te maken met maar één verklarende variabele dan wordt dat *enkelvoudige lineaire regressie* genoemd. Zijn er meerdere verklarende variabelen dan heet dat *meervoudige lineaire regressie*.

Hoewel regressie en correlatie vaak samen gebruikt worden, zijn het twee verschillende begrippen.

+ *Regressie* heeft als doel om de soort relatie tussen $y$ en $x$ te bepalen. Bijvoorbeeld $y$ neemt lineair toe als $x$ toeneemt.
+ *Correlatie* meet de sterkte van die relatie. Is die sterk of juist zwak.

:::{important}
Regressie en correlatie verklaren de wijzigingen in de afhankelijke variabele, maar niet de oorzaak van deze wijzigingen. Er hoeft dus geen oorzakelijk verband te zijn.
:::

## Enkelvoudige lineaire regressie  {#linreg-enkelvoudig}

Het lineaire regressiemodel bevat één verklarende variabele en kun je weergeven met de vergelijking

$y = \beta_0 + \beta_1 * x + \epsilon$, met

+ $y$ : afhankelijke (respons) variabele
+ $x$ : onafhankelijke (verklarende) variabele
+ $\beta_0$ : intercept, snijpunt met y-as, de waarde van $y$ voor $x=0$
+ $\beta_1$ : richtingscoëfficient, de verandering van de waarde van $y$ wanneer de waarde van $x$ met 1 toeneemt.
+ $\epsilon$ : foutterm, verschil tussen de echte waarde van $y$ en de geschatte waarde van $y$. Dit is het deel van de afhankelijke variabele dat niet verklaard kan worden door de onafhankelijke variabele. Wordt ook wel ruis of storing genoemd.

Bij lineaire regressie probeer je de beste waarden voor $\beta_0$ en $\beta_1$ te vinden zodat de totale fout van het model zo klein mogelijk wordt.

Als voorbeeld wordt een gesimuleerde dataset met 15 waarnemingen voor $x$ en $y$ gebruikt om een aantal achtergronden uit te leggen.

```{r sim1-data, echo=FALSE}
sim1.data <- read_csv2("data/sim1.csv")
sim1.data %>% 
	t() %>% 
	kbl(digits = 1, format.args = list(decimal.mark = ","), caption = "Dataset met 15 waarnemingen x en y.") %>% 
	kable_styling(full_width = FALSE, font_size = 14)
```

In figuur \@ref(fig:sim1-basis) kun je zien hoe ze aan elkaar gerelateerd zijn.

```{r sim1-basis, fig.cap="Spreidingsdiagram voor de 15 waarnemingen.", echo=FALSE}
ggplot(data = sim1.data, aes(x, y)) +
	geom_point(colour = "black") +
	theme_bw()
```

De gegevens laten een sterk lineair patroon zien en lenen zich ervoor om het patroon in een lineair model vast te leggen. Wanneer je de waarden voor $\beta_0$ en $\beta_1$ hebt, kun je via de vergelijking van de regressielijn voor elke waarde van $x$ een bijbehorende waarde van $y$ berekenen. Deze voorspelde y-waarden worden ook wel de geschatte waarden genoemd.

De kunst is nu om zodanige waarden voor $\beta_0$ en $\beta_1$ te vinden, zodat de regressielijn zo dicht mogelijk bij de gegevens ligt, dan ligt de voorspelde waarde zo dicht mogelijk bij de echte waarde.

In figuur \@ref(fig:sim1-regressie) is een regressielijn (rood) getekend met daarnaast vertikale afstanden (blauwe lijnstukjes) tussen de echte y-waarde (de zwarte punt) en de voorspelde y-waarde (het punt op de rode lijn.

```{r sim1-regressie, fig.cap="Spreidingsdiagram nu met regressielijn en afstandslijntjes.", echo=FALSE}
sim1.data %>% 
	mutate(prog = 4.71 + 2.05*x) %>% 
	ggplot(aes(x, y)) + 
	geom_abline(intercept = 4.71, slope = 2.05, colour = "red") +
 	geom_point(colour = "black") + 
 	geom_linerange(aes(ymin = y, ymax = prog), colour = "blue") + 
 	theme_bw()
```

Voor elk punt is afstand = $y - y_{voorspeld}$. Dit verschil wordt ook wel **residu** genoemd. En de beste lijn is de lijn waarvoor de som van de kwadratische afstanden zo klein mogelijk is. Deze methode heet ook wel de **kleinste kwadraten methode**. Hoe je dit precies berekend is in dit kader niet van belang. Excel gebruikt deze methode ook.

Je kunt in Excel op een aantal manieren een lineaire regressie uitvoeren.

### Methode - Formules

De regresiecoëfficienten kun je met de volgende formules bepalen:

+ $\beta_0$ - met formule `SNIJPUNT(y-bekend ; x-bekend)`
+ $\beta_1$ - met formule `RICHTING(y-bekend ; x-bekend)`

Toegepast op het voorbeeld levert dit op: `SNIJPUNT=4,7096` en `RICHTING= 2,0549`. De regressielijn wordt dan $y = 4,7096 + 2,0549 * x$.

Verdere analyses, zoals het berekenen van de residuen, zul je zelf moeten samenstellen met behulp van deze waarden.

### Methode - Spreidingsdiagram met trendlijn

Bij het maken van een spreidingsdiagram kun je bij de opmaak aangeven dat de vergelijking van de lijn in de grafiek moet worden weergegeven. Je krijgt dan de gevonden waarden voor $\beta_0$ en $\beta_1$ te zien. Voor visualisatie is dit wel nuttig, maar verder kun je hier niets mee doen.

```{r sim1-spreiding, fig.cap="Spreidingsdiagram in Excel en vergelijking lijn."}
knitr::include_graphics("images/linregressie/sim1-spreiding.png")
```

### Methode - Gegevensanalyse

Met deze methode kun je een uitvoerige analyse maken. Kies *Gegevens > Gegevensanalyse > Regressie*. Je krijgt dan het volgende dialoogscherm:

```{r toolpak-regressie, fig.cap="Dialoogscherm regressie."}
knitr::include_graphics("images/linregressie/toolpak-regressie.png")
```

+ *Invoerbereik y* - Specificeer het gebied voor de y-waarden, inclusief eventuele koptekst.
+ *Invoerbereik x* - Specificeer het gebied voor de x-waarden, inclusief eventuele koptekst.
+ *Labels* - Selecteer deze wanneer de invoerbereiken kopteksten bevatten.
+ *Betrouwbaarheidsniveau* - Deze staat standaard op 95% en kun je in de meeste gevallen zo laten. Desgewenst kan het via deze keuzeknop gewijzigd worden in een ander percentage.
+ *Uitvoeropties* - Geef aan waar de uitvoer moet komen.
+ *Storingen* - Storingen zijn de residuen. Voor elke waarneming wordt de storing bepaald.
+ *Standaardstoringen* - De standaardwaarde van elke storing.
+ *Grafiek voor storingen* - Spreidingsdiagram van storing en $x$.
+ *Grafiek voor lijnen* - Spreidingsdiagram waarin zowel $y$ als $y_{voorspeld}$ uitgezet is tegen $x$.

Na OK verschijnt de uitvoer van de regressie-analyse.

De uitvoer bestaat uit een aantal onderdelen. De belangrijkste daarvan zullen hier besproken worden.

**Samenvatting regressie**

```{r sim1-samenvatting, echo=FALSE, fig.cap="Samenvatting regressie."}
knitr::include_graphics("images/linregressie/sim1-samenvatting.png")
```

+ *R-kwadraat* - Een belangrijke waarde welke aangeeft hoeveel van de variantie in de afhankelijke variabele verklaard wordt door de variantie in de onafhankelijke variabele(n). Dus in feite hoe goed de regressielijn bij de werkelijke data past. De waarde ligt tussen 0 1n 1. Bij een waarde van 0,7 (70%) heb je al een redelijk goede waarde.
+ *Aangepaste kleinste kwadraat* - Wat conservatiever dan R-kwadraat.

**Variantie-analyse (ANOVA)**

```{r sim1-anova, fig.cap="Variantie-analyse regressie."}
knitr::include_graphics("images/linregressie/sim1-anova.png")
```

Bij eenvoudige lineaire regressie is dit niet zo van belang. De waarde van *Significantie F* moet wel groter dan 0,05 zijn, zoniet onderzoek dan of een andere onafhankelijke variabele geschikter is.

**Regressie-coëfficiënten**

```{r sim1-coefficienten, fig.cap="Coéfficiënten regressielijn."}
knitr::include_graphics("images/linregressie/sim1-coefficienten.png")
```

Dit is wel het belangrijkste deel in de uitvoer. Je ziet de reeds eerder genoemde waarden van de coëfficiënten van de regressievergelijking: $y = 4,7096 + 2,0549 * x$.

**Residuen**

De residuen zijn het verschil tussen de werkelijke waarde en de voorspelde waarde en zijn eenvoudig uit te rekenen. Zo is voor de 7e waarneming met $x = 4,0$ en $y = 14,3$ de voorspelde waarde $y_{voorspeld} = 4,7096 + 2,0549*4,0 = 12,93$ en dus is het residu $\epsilon = 14,3 - 12,93 = 1,37$.

Gelukkig hoef je dit niet allemaal handmatig uit te rekenen. *Storingen* is de term die Excel gebruikt voor *Residuen*. Wanneer je in het dialoogscherm Gegevensanalyse hebt aangegeven dat "Storingen" en "Grafiek voor storingen" moeten worden weergegeven  dan wordt het volgende in de uitvoer getoond. 

```{r sim1-residu, out.width="40%", fig.cap="Residu Analyse."}
knitr::include_graphics("images/linregressie/sim1-residuen.png")
knitr::include_graphics("images/linregressie/sim1-residuplot.png")
```

In de grafiek in figuur \@ref(fig:sim1-residu) zijn de residuen (storingen) uitgezet tegen de bijbehorende x-waarde. In het ideale geval zouden alle residuen gelijk aan 0 moeten zijn en de de punten in het residuendiagram op de x-as moeten liggen. De residuen vormen dus de variatie in de data die niet door het regressiemodel verklaard worden. Bij een positief residu wordt de waarde *onderschat* door het model en bij een negatief residu wordt de waarde *overschat*.

:::{ .important}
**Residuen** zijn belangrijk bij het evalueren hoe goed een (lineair) model bij de dataset past. Het diagram met residuen moet altijd goed bestudeerd worden.
Bij een goed model moeten de residuen willekeurig langs de x-as verdeeld zijn en moet het gemiddelde en de som van de residuen bij benadering zo dicht mogelijk bij nul liggen. Is dat niet het geval en is er nog een patroon zichtbaar, dan moet het model aangepast worden. In het hierna volgende voorbeeld wordt dat duidelijk gemaakt.
:::

### Voorbeeld Druk-Temperatuur {#sim2}

Bij een bepaald natuurkundig experiment wil men onderzoeken of de druk lineair afhankelijk is van de temperatuur. Daartoe zijn 10 waarnemingen verzameld die te vinden zijn in het bestand [sim2.csv](data/sim2.csv).


```{r sim2-data, echo=FALSE}
sim2.data <- read_csv2("data/sim2.csv")
sim2.data %>% 
	kbl(digits = 1, format.args = list(decimal.mark = ","), caption = "") %>% 
	kable_styling(bootstrap_options = "condensed", full_width = FALSE, font_size = 12)
```

In figuur \@ref(fig:sim2-spreiding) is de druk tegen de temperatuur uitgezet.

```{r sim2-spreiding, fig.cap="Spreidingsdiagram in Excel met vergelijking lijn."}
knitr::include_graphics("images/linregressie/sim2-spreiding.png")
```

Er is een duidelijk positief verband te zien dat redelijk bij een rechte lijn in de buurt komt. Echter in het midden is wel een vorm van een kromming waar te nemen.

Vervolgens wordt een regressie-analyse via Gegevensanalyse uitgevoerd.

```{r sim2-samenvatting, fig.cap="Samenvatting regressie."}
knitr::include_graphics("images/linregressie/sim2-samenvatting.png")
```

Zowel de correlatiecoëfficiënt liggen dicht bij 1 en zijn dus goed.

```{r sim2-coefficienten, fig.cap="Coéfficiënten regressielijn."}
knitr::include_graphics("images/linregressie/sim2-coefficienten.png")
```

Voor beide coëfficiënten is de p-waarde kleiner dan 0,05 en zijn dus de coëfficiënten significant.

```{r sim2-residu, out.width="40%", fig.cap="Residu Analyse."}
knitr::include_graphics("images/linregressie/sim2-residuen.png")
knitr::include_graphics("images/linregressie/sim2-residuplot.png")
```

Het gemiddelde en de som van de residuen zijn naderhand met een formule berekend en zijn nagenoeg nul, wat ook goed is.

Echter in het diagram van de residuen is te zien dat de waarden niet willekeurig verdeeld zijn en een patroon laten zien dat op een parabool lijkt. Dat betekent dat er misschien een verborgen patroon is dat niet door het lineaire model wordt meegenomen. Vanwege het paraboolkarakter van de residuen ligt het voor de hand om te onderzoeken of het toevoegen van een kwadratische term een beter een resultaat oplevert. Het model zou dan de volgende vorm kunnen hebben:

$druk = \beta_0 + \beta_1 * temperatuur + \beta2 * temperatuur^2 + \epsilon$

Het valt buiten het karakter van dit hoofdstuk om deze analyse uit te voeren. Wel kun je in het spreidingsdiagram voor een andere trendlijn dan lineair kiezen. Zo levert de keuze voor een polynoom van de graad 2 het volgende op.

```{r sim2-polynoom2, fig.cap="Spreidingsdiagram met een polynoom van de graad 2 als trendlijn."}
knitr::include_graphics("images/linregressie/sim2-polynoom2.png")
```

De R-kwadraat is nu bijna 1.

## Meervoudige lineaire regressie {#linreg-meervoudig}

Bij een meervoudig lineair regressiemodel zijn er meerdere verklarende variabelen. De algemene vergelijking is

$y = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 + \beta_3 * x_3 + \cdots  + \epsilon$, met

+ $y$ : afhankelijke (respons) variabele
+ $x_1, x_2, x_3, \cdots$ : onafhankelijke (verklarende) variabelen

Om het overzichtelijk en niet te ingewikkeld te maken, een voorbeeld met een gefingeerde verzameling economische gegevens. Deze data kun je in Excel importeren via het bestand [economiedata.xlsx](data/economiedata.xlsx).

```{r ecdata, fig.cap="Dataset economische gegevens."}
knitr::include_graphics("images/linregressie/ecdata.png")
```

Onderzocht gaat worden of `Aandelenindex` lineair afhankelijk is van zowel `Rentepercentage` als `Werkloosheidspercentage`. De variabelen zijn dus:

+ Afhankelijke variabele: `Aandelenindex`
+ Onafhankelijke variabelen: `Rentepercentage` en `Werkloosheidspercentage`

Allereerst wordt gekeken of er een relatie is tussen de afhankelijke variabele en elk van de onafhankelijke variabelen afzonderlijk.

```{r ecdata-relaties, out.width="50%", fig.cap="Relaties tussen Aandelenindex en respectievelijk Rentepercentage en Werkloosheidspercentage."}
knitr::include_graphics("images/linregressie/ecdata-rel1.png")
knitr::include_graphics("images/linregressie/ecdata-rel2.png")
```

1. *Aandelenindex - Rentepercentage* - Er is een lineair verband te zien. Als het `Rentepercentage` omhoog gaat, gaat ook de `Aandelenindex` omhoog. Er is een positief verband. En een voldoend hoge R-kwadraat van 0,8757.

2. *Aandelenindex - Werkloosheidspercentage* - Er is een lineair verband te zien. Als het `Werkloosheidspercentage` omhoog gaat, gaat de `Aandelenindex` omlaag. Er is een negatief verband. En een voldoend hoge R-kwadraat van 0,8507.

Er wordt nu een meervoudig lineair model opgesteld met de formule

$Aandelenindex = \beta_0 + \beta_1 * Renteperc + \beta_2 * Werkloosheidsperc  + \epsilon$

Bij het dialoogscherm Regressie in Gegevensanalyse moet je er wel op letten dat je in het invoerbereik voor X beide onafhankelijke variabelen opneemt, zie figuur \@ref(fig:ecdata-regressie).


```{r ecdata-regressie, fig.cap="Dialoogscherm regressie voor economiedata."}
knitr::include_graphics("images/linregressie/ecdata-regressie.png")
```

Vervolgens wordt een regressie-analyse via Gegevensanalyse uitgevoerd.

```{r ecdata-samenvatting, fig.cap="Samenvatting regressie meervoudige lineaire regressie."}
knitr::include_graphics("images/linregressie/ecdata-samenvatting.png")
```

Zowel de correlatiecoëfficient als R-kwadraat zijn in dit meervoudige model hoger dan voor de afzonderlijke relaties. Dicht bij 1, dus goed.

```{r ecdata-coefficienten, fig.cap="Coéfficiënten regressievergelijking."}
knitr::include_graphics("images/linregressie/ecdata-coefficienten.png")
```

De regressievergelijking wordt $Aandelenindex = 1798,40 + 345,54 * Rentepercentage - 250,15 * Werkloosheidspercentage  + \epsilon$

Alleen de p-waarde voor het Rentepercentage is flink beneden 0,05. Voor het snijpunt en werkloosheidspercentage is deze respectievelijk iets er boven en iets eronder. Het model kan dus waarschijnlijk nog wel verbeterd worden.

Het gemiddelde en de som van de residuen is nagenoeg nul, wat dus weer goed is. Voor de diagrammen van de residuen


```{r ecdata-residu, out.width="50%", fig.cap="Diagrammen van de residuen."}
knitr::include_graphics("images/linregressie/ecdata-residu1.png")
knitr::include_graphics("images/linregressie/ecdata-residu2.png")
```

Bij beide diagrammen zijn de residuen willekeurig verdeeld en is er geen patroon waar te nemen.

Samenvattend is het meervoudige lineaire model een redelijk model. Onderzocht zou moeten worden of er nog verbetering vatbaar is. Zo ligt het voor de hand om hier een mogelijke interactie tussen de onafhankelijke variabelen in het model mee te nemen. Bij een model met twee onafhankelijke variabelen ziet het model er dan als volgt uit:

$y = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2 + \beta_3 * x_1 * x_2 + \epsilon$

Naast dat dit in Excel veel handwerk met zich meebrengt, is dit ook geen lineair model meer en valt een uitwerking hiervan buiten het bestek van dit boek.

## Voorspellen

Wanneer je eenmaal een betrouwbaar regressiemodel gemaakt hebt, dan kun je hiermee de waarde van de afhankelijke variabele schatten, door de waarden voor de onafhankelijke variabelen in de regressievergelijking in te vullen.

Hierbij is wel een kanttekening te maken. Modellen hebben echte beperkingen. Het lineaire regressiemodel is opgesteld voor een bepaald bereik van de onafhankelijke variabelen. Je weet niet hoe de gegevens zich buiten dat bereik gaan gedragen. Het toepassen van een model voor een schatting op waarden buiten het het bereik van de oorspronkelijke gegevens wordt **extrapolatie** genoemd. Over het algemeen is een lineair model slechts een benadering van de werkelijke relatie tussen twee variabelen. En als je dan gaat extrapoleren, dan maak je een onbetrouwbare gok dat deze relatie ook geldig is op plaatsen waar deze niet is geanalyseerd.

Ter toelichting een voorbeeld. De gevonden vergelijking bij de meervoudige lineaire regresie, zie \@ref(linreg-meervoudig), luidt

$Aandelenindex = 1798,40 + 345,54 * Rentepercentage - 250,15 * Werkloosheidspercentage$

Bij een werkloosheidspercentage van 6,0 en een negatieve rente (niet ondenkbaar) van -0,5 is de aandelenindex 125. En bij een negatieve rente van -1,0 zou de aandelenindex zelfs -48 zijn!! Dat kan natuurlijk niet.

## Opgaven {#opg-linreg}

::: {.sectionexercise}
1. In \@ref(linreg-enkelvoudig) is een regressie-analyse besproken met de data in tabel \@ref(tab:sim1-data). Voer deze analyse in Excel uit. Experimenteer met de uitvoeropties die Gegevensanalyse biedt. De dataset kun je ook downloaden als [sim1.csv](data/sim1.csv).

1. Voer in Excel de analyse uit het voorbeeld \@ref(sim2) uit. De dataset kun je ook downloaden als [sim2.csv](data/sim2.csv).

1. In het bestand [salaris_data.csv](data/salaris_data.csv) is van 30 personen het salaris en het aantal jaren ervaring verzameld. Stel hiervoor een lineair regressiemodel op.

1. Een fabrikant van energierepen wil de dalende omzet van het topprodukt "GreenBull" weer een impuls geven. Om een plan hiervoor te ontwikkelen zijn via een steekproef in 34 winkels een aantal gegevens verzameld, welke te vinden zijn in het bestand [energiereep.xlsx](data/energiereep.xlsx). Stel hiervoor een meervoudig lineair regressiemodel op. De verzamelde gegevens zijn:

    - `winkelnr`
    - `omzet` - aantal verkochte repen per maand in de winkel
    - `prijs` - verkoopprijs reep (centen)
    - `promotiekosten` - maandelijkse kosten voor promotie van het produkt in de winkel (euros)

:::
